---
layout: page
title: 
permalink: "/urops/"
usemathjax: true
---

{% raw %}

 <style>
    table {
        width: 100%;
        border-collapse: collapse;
        table-layout: fixed;
    }
    th, td {
        border: 0px solid black;
        padding: 10px;
        text-align: center;
        width: 25%;
    }
</style>
{% endraw %}

Cambridge University UROPs 2025: Small Language Models Research Projects
<body> 



<table>
    <tr>
        <th>Week</th>
        <th>Topic</th>
        <th>Course Materials</th>
    </tr>
      <tr>
        <td colspan="3">
            <strong>Small Langauge Models:</strong><br>
            <a href="https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_01.json">CS335 Language Models from Scratch</a><br>
            <a href="https://web.stanford.edu/class/cs224u/slides/cs224u-contextualreps-2023-handout.pdf">CS224U Contextual Word Representations</a><br>
            <a href="https://web.stanford.edu/class/cs224u/slides/lisa-224u-diffusion.pdf">CS224U Diffusion Models for Text</a><br>
            <a href="https://web.stanford.edu/class/cs224u/slides/cs224u-incontextlearning-2023-handout.pdf">CS224U In-Context Learning</a><br>
            <a href="https://arxiv.org/pdf/2411.03350">A Comprehensive Survey of Small Language Models in the Era of Large Language Models</a><br>
            <a href="https://arxiv.org/pdf/2506.02153">Small Language Models are the Future of Agentic AI</a>
        </td>
    </tr>
      <tr>
        <td>1</td>
        <td>BabyLMs and Multilingual Evaluation</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Suggested Readings:</strong><br>
            <a href="#">GloVe: Global Vectors for Word Representation</a> (original GloVe paper)<br>
            <a href="#">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a><br>
            <a href="#">Evaluation methods for unsupervised word embeddings</a><br>
            <strong>Additional Readings:</strong><br>
            <a href="#">A Latent Variable Model Approach to PMI-based Word Embeddings</a><br>
            <a href="#">Linear Algebraic Structure of Word Senses, with Applications to Polysemy</a><br>
            <a href="#">On the Dimensionality of Word Embedding</a>
        </td>
    </tr>
</table>


</body> 

<h3 class="font-weight-bold mb-4 serif-font">Useful Links</h3>



 


