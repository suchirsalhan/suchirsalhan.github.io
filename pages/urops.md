---
layout: page
title: 
permalink: "/urops/"
usemathjax: true
---

{% raw %}

 <style>
    table {
        width: 100%;
        border-collapse: collapse;
        table-layout: fixed;
    }
    th, td {
        border: 0px solid black;
        padding: 10px;
        text-align: center;
        width: 25%;
    }
</style>
{% endraw %}

<h3>Cambridge University UROPs 2025: Small Language Models Research Projects</h3>

<body> 



<table>
    <tr>
        <th>Week</th>
        <th>Topic</th>
        <th>Course Materials</th>
    </tr>
      <tr>
        <td colspan="3">
            <strong>Small Language Models:</strong><br>
            <a href="https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_01.json">CS335 Language Models from Scratch</a><br>
            <a href="https://web.stanford.edu/class/cs224u/slides/cs224u-contextualreps-2023-handout.pdf">CS224U Contextual Word Representations</a><br>
            <a href="https://web.stanford.edu/class/cs224u/slides/lisa-224u-diffusion.pdf">CS224U Diffusion Models for Text</a><br>
            <a href="https://web.stanford.edu/class/cs224u/slides/cs224u-incontextlearning-2023-handout.pdf">CS224U In-Context Learning</a><br>
            <a href="https://arxiv.org/pdf/2411.03350">A Comprehensive Survey of Small Language Models in the Era of Large Language Models</a><br>
            <a href="https://arxiv.org/pdf/2506.02153">Small Language Models are the Future of Agentic AI</a>
        </td>
    </tr>
      <tr>
        <td>1</td>
        <td>BabyLMs and Multilingual Evaluation</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://aclanthology.org/2023.conll-babylm.1.pdf"> Findings of the 1st BabyLM Challenge </a><br>
            <a href="https://aclanthology.org/2023.conll-babylm.1.pdf"> Findings of the 2nd BabyLM Challenge </a><br>
            <a href="https://suchirsalhan.github.io/assets/papers/L95_BLiMP.pdf"> Handout on Language Model Evaluation</a> (Suchir Salhan, L95 MPhil ACS)<br>
            <strong>Recommended Additional Readings:</strong><br>
            <a href="https://aclanthology.org/2023.conll-babylm.10/"> CLIMB â€“ Curriculum Learning for Infant-inspired Model Building </a><br>
            <a href="[https://aclanthology.org/2023.conll-babylm.10/"> Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies </a><br>
         <strong>Practical Tasks:</strong><br>
         TO DO <br>
        </td>
    </tr>

 <tr>
         <td>2</td>
        <td>Bilingual BabyLM Training and Evaluation</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://arxiv.org/pdf/2503.03962"> On the Acquisition of Shared Grammatical Representations in Bilingual Language Models </a><br>
            <a href="https://arxiv.org/abs/2504.02768"> MultiBLiMP 1.0 </a><br>
            <strong>Recommended Additional Readings:</strong><br>
             <a href="https://universaldependencies.org/"> Universal Dependencies </a><br>
         <strong>Practical Tasks:</strong><br>
         TO DO <br>
        </td>
    </tr> 

 <tr>
         <td>3</td>
        <td> Pretraining Language Models: The Pico Framework</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://www.picolm.io/guides/train"> Pico Train Tutorial </a><br>
            <a href="https://www.picolm.io/guides/analyze"> Pico Analyze Tutorial </a><br>
            <strong>Recommended Additional Readings:</strong><br>
            TO DO <br>
         <strong>Practical Tasks:</strong><br>
         TO DO <br>
        </td>
    </tr> 


 <tr>
         <td>4</td>
        <td> BabyLM Architectures & Feedback (ALTA CST)</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://arxiv.org/abs/2308.02019"> BabyLLama </a><br>
            TO DO <br>
         <strong>Practical Tasks:</strong><br>
         TO DO <br>
        </td>
    </tr> 

 <tr>
         <td>4</td>
        <td> Mechanistic and Developmental Interpretability</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://arxiv.org/pdf/2311.03658"> The Linear Representation Hypothesis and the Geometry of Large Language Models</a><br>
            <strong>Additional Readings:</strong><br>
            <a href="https://drive.google.com/file/d/1yaBGi6RQdcGlGfjjGLGf5fo0TaINpsLm/view?usp=sharing"> Slides from Arthur Conmy (Google DeepMind)</a><br>
         <strong>Practical Tasks:</strong><br>
         TO DO <br>
        </td>
    </tr> 


</table>


</body> 

<h3 class="font-weight-bold mb-4 serif-font">Useful Links</h3>



 


