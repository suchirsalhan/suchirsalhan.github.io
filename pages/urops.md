---
layout: page
title: 
permalink: "/urops/"
usemathjax: true
---

{% raw %}
<style>
    table {
        width: 100%;
        border-collapse: collapse;
        table-layout: fixed;
    }
    th, td {
        padding: 10px;
        text-align: center;
        width: 25%;
        border-bottom: 1px solid #ccc;
    }
    tr:nth-child(even) {
        background-color: #FFFFC5; /* Light Yellow for even rows */
    }
    tr:nth-child(odd) {
        background-color: #F0FFF0; /* Light Green for Odd Rows */
    }
    tr:last-child td {
        border-bottom: none;
    }
</style>
{% endraw %}



<h1>Cambridge University UROPs 2025: Small Language Models Research Projects</h1>

<table>
    <tr>
        <th>Week</th>
        <th>Topic</th>
        <th>Course Materials</th>
    </tr>
    <tr>
        <td>1 (14/07)</td>
        <td>BabyLMs and Multilingual Evaluation</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://aclanthology.org/2023.conll-babylm.1.pdf">Findings of the 1st BabyLM Challenge</a><br>
            <a href="https://aclanthology.org/2023.conll-babylm.2.pdf">Findings of the 2nd BabyLM Challenge</a><br>
            <a href="https://suchirsalhan.github.io/assets/papers/L95_BLiMP.pdf">Handout on Language Model Evaluation</a><br>
            <strong>Recommended Additional Readings:</strong><br>
            <a href="https://aclanthology.org/2023.conll-babylm.10/">CLIMB – Curriculum Learning for Infant-inspired Model Building</a><br>
            <a href="https://aclanthology.org/2023.conll-babylm.11/">Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies</a><br>
            <strong>Practical Tasks:</strong><br>
            TO DO<br>
        </td>
    </tr>

    <tr>
        <td>2 (21/07)</td>
        <td>Bilingual BabyLM Training and Evaluation</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://arxiv.org/pdf/2503.03962">On the Acquisition of Shared Grammatical Representations in Bilingual Language Models</a><br>
            <a href="https://arxiv.org/abs/2504.02768">MultiBLiMP 1.0</a><br>
            <strong>Recommended Additional Readings:</strong><br>
            <a href="https://universaldependencies.org/">Universal Dependencies</a><br>
            <strong>Practical Tasks:</strong><br>
            TO DO<br>
        </td>
    </tr>

    <tr>
        <td>3 (28/07)</td>
        <td>Pretraining Language Models: The Pico Framework</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://www.picolm.io/guides/train">Pico Train Tutorial</a><br>
            <a href="https://www.picolm.io/guides/analyze">Pico Analyze Tutorial</a><br>
            <strong>Recommended Additional Readings:</strong><br>
            TO DO<br>
            <strong>Practical Tasks:</strong><br>
            TO DO<br>
        </td>
    </tr>

    <tr>
        <td>4 (04/08)</td>
        <td>BabyLM Architectures & Feedback (ALTA CST)</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://arxiv.org/abs/2308.02019">BabyLLama</a><br>
            TO DO<br>
            <strong>Practical Tasks:</strong><br>
            TO DO<br>
        </td>
    </tr>

    <tr>
        <td>5 (11/08)</td>
        <td>Mechanistic and Developmental Interpretability</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://arxiv.org/pdf/2311.03658">The Linear Representation Hypothesis and the Geometry of Large Language Models</a><br>
            <strong>Additional Readings:</strong><br>
            <a href="https://drive.google.com/file/d/1yaBGi6RQdcGlGfjjGLGf5fo0TaINpsLm/view?usp=sharing">Slides from Arthur Conmy (Google DeepMind)</a><br>
            <strong>Practical Tasks:</strong><br>
            <strong>Interim Project Presentation</strong><br>
        </td>
    </tr>

    <tr>
        <td>6 (18/08)</td>
        <td>Train Your Own BabyLM From Scratch</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Interaction Track</strong><br>
            <strong>Multimodal Track</strong><br>
            <strong>Required Readings:</strong><br>
            TO DO<br>
            <strong>Practical Tasks:</strong><br>
            TO DO<br>
        </td>
    </tr>

    <tr>
        <td>7 (25/08)</td>
        <td>Small Language Models – Frontier Problems</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://arxiv.org/pdf/2311.03658">The Linear Representation Hypothesis and the Geometry of Large Language Models</a><br>
            <strong>Additional Readings:</strong><br>
            <a href="https://drive.google.com/file/d/1yaBGi6RQdcGlGfjjGLGf5fo0TaINpsLm/view?usp=sharing">Slides from Arthur Conmy (Google DeepMind)</a><br>
            <strong>Practical Tasks:</strong><br>
            TO DO<br>
        </td>
    </tr>

    <tr>
        <td>8 (01/09)</td>
        <td>Small Language Models – Frontier Problems (Architectures)</td>
        <td>
            <a href="#">Slides</a> | <a href="#">Notes</a> | <a href="#">Code</a><br>
            <strong>Required Readings:</strong><br>
            <a href="https://arxiv.org/pdf/2311.03658">The Linear Representation Hypothesis and the Geometry of Large Language Models</a><br>
            <strong>Additional Readings:</strong><br>
            <a href="https://drive.google.com/file/d/1yaBGi6RQdcGlGfjjGLGf5fo0TaINpsLm/view?usp=sharing">Slides from Arthur Conmy (Google DeepMind)</a><br>
            <strong>Practical Tasks:</strong><br>
            <strong>Final Project Presentations and UROP Project Reports (Friday 5 September 2025, Due 5pm)</strong><br>
        </td>
    </tr>
</table>

<h3 class="font-weight-bold mb-4 serif-font">Language Model Primers</h3>
<ul>
    <li><a href="https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_01.json"><u>CS336 Language Models from Scratch</u></a></li>
    <li><a href="https://web.stanford.edu/class/cs224u/slides/cs224u-contextualreps-2023-handout.pdf"><u>CS224U Contextual Word Representations</u></a></li>
    <li><a href="https://web.stanford.edu/class/cs224u/slides/cs224u-incontextlearning-2023-handout.pdf"><u>CS224U In-Context Learning</u></a></li>
</ul>

<h3 class="font-weight-bold mb-4 serif-font">Small Language Models</h3>
<ul>
    <li><a href="https://web.stanford.edu/class/cs224u/slides/lisa-224u-diffusion.pdf"><u>CS224U Diffusion Models for Text</u></a></li>
    <li><a href="https://arxiv.org/pdf/2411.03350"><u>A Comprehensive Survey of Small Language Models in the Era of Large Language Models</u></a></li>
    <li><a href="https://arxiv.org/pdf/2506.02153"><u>Small Language Models are the Future of Agentic AI</u></a></li>
</ul>

<h3 class="font-weight-bold mb-4 serif-font">Find Out More!</h3>

<strong>Tokenisers</strong><br>
<strong>Non-Autoregressive Language Models</strong>
<ul>
    <li><a href="https://web.stanford.edu/class/cs224u/slides/lisa-224u-diffusion.pdf"><u>CS224U Diffusion Models for Text</u></a></li>
    <li><a href="https://spacehunterinf.github.io/blog/2025/diffusion-language-models/"><u>What are Diffusion Language Models? – Blogpost by Xiaochen Zhu (NLIP Group, PhD Student)</u></a></li>
</ul>

<h3 class="font-weight-bold mb-4 serif-font">Useful Links (Raven Access Required)</h3>
<p>
See Dr Andrew Caines's page for lots of useful advice, particularly about access to compute resources. <br>
Dr Russell Moore previously developed his ML Commando Course for the 2021 ALTA UROPs, which you might find helpful.
</p>

