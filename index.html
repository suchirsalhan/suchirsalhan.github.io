<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Suchir Salhan</title>
  <meta name="author" content="Suchir Salhan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/style.css" rel="stylesheet">
  <link href="css/font.css" rel="stylesheet">  
  <link rel="icon" href="sas245.jpg">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4VJBBSSEN8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-4VJBBSSEN8');
  </script>
</head>

<body style="background-color: #0d0d0d;">
  <div class="center">
    <header>
      <h1>Suchir Salhan</h1>
        <nav class="navbar">
        <a href="https://suchirsalhan.github.io/slm" target="_blank">Cambridge Small LMs</a> 
      </nav>
    </header>


    <div class="spacer"></div>
    
    <h1 style="margin-top: 0; font-size: 2.5rem">
      Computer Science PhD at <br class="ignore-on-mobile"/> University of Cambridge
    </h1>
    <img src="images/computer-lab.jpg" alt="Picture of the William Gates Building." class="hero">


      <main>
      <section class="group-section">
        <h3 >Academic Interests</h2>
        <ul>
          <li >Natural Language Processing</li>
          <li>Machine Learning</li>
        </ul>
        <p><a href="https://suchirsalhan.github.io/slm" target="_blank"> Learn more about Cambridge Small Language Models</a></p> <!-- New link added here -->
        <ul style="list-style-type:none; padding:0; margin:0; width:100%; max-width:800px; margin-right:auto; margin-left:auto;">
    <li>
        <div style="padding:2.5%; vertical-align:middle; text-align:center;">
            <img src="images/sas245.jpg" alt="Suchir Salhan" style="max-width:100%; height:auto;">
            <p>I am a PhD Candidate in Computer Science at the <a href="https://www.cst.cam.ac.uk/research/themes/natural-language-processing">University of Cambridge</a> (Gonville & Caius College), researching Machine Learning and Natural Language Processing and supervised by <a href="https://www.cst.cam.ac.uk/people/pjb48">Professor Paula Buttery</a>. I specialise in Machine Learning and Natural Language Processing. I am interested in developing cognitively-inspired computational systems, including alternatives to Transformer-based Large Language Models. I previously completed my Bachelor of Arts and Masters in Engineering in Computer Science and Linguistics at Gonville & Caius College, University of Cambridge, where I obtained a Starred First (Class I with Distinction) and a Distinction (also equivalent to a starred First) respectively. My research focuses on Small-Scale Language Models to improve the interpretability of Foundation Models.</p>
            <p>My research primarily is concerned with engineering more cognitively plausible Foundation Models. This is an emerging research paradigm that attempts to improve the cognitive capabilities of state-of-the-art computational systems within a cognitively plausible environment. I have interests in Machine Learning Systems, the Theory of Deep Learning, and Theoretical Linguistics. My ambition is to develop data-efficient Machine Learning systems that draw on human cognition.</p>
            <p>To this end, I'm particularly interested in developing novel machine learning techniques to build scalable neural architectures that draw on formal methods (e.g., category and type theory) utilised in theoretical formalisms in Cognitive Science.</p>
            <p style="text-align:center;">
                <a href="mailto:sas245@cam.ac.uk">Email</a> &nbsp;/&nbsp;
                <a href="data/cv.pdf">CV</a> &nbsp;/&nbsp;
                <a href="https://twitter.com/suchirsalhan/">Twitter</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/ssalhan">LinkedIn</a> &nbsp;/&nbsp;
                <a href="https://github.com/suchirsalhan/">Github</a>
            </p>
        </div>
    </li>
</ul>
      </section>


      
      <section>
        <h2>Publications</h2>
        <ul>
          <li>
            <div class="card-base card-round-top">
              <div class="card-content">
                <div class="card-header">
                  <div class="card-header-title">
                    <h3>Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies</h3>
                    <h3>CoNLL BabyLM Challenge</h3>
                  </div>
                  <img src="img/conll_logo.png" alt="CoNLL Logo" class="work-logo" style="width: 8rem">
                </div>
                <div class="card-body">
                  <p class="card-body-title">2024</p>
                </div>
                <div class="card-body-content">
                  <p><strong>Authors:</strong> Suchir Salhan, Richard Diehl-Martinez, Zebulon Goriely, Paula Buttery</p>
                  <p>This publication introduces Small-Scale Language Models (SSLMs) trained on developmentally plausible corpora. It explores cognitive curriculum learning strategies that provide insights into syntactic learning across different languages.</p>
                </div>
              </div>
            </div>
          </li>
          <li>
            <div class="card-base">
              <div class="card-content">
                <div class="card-header">
                  <div class="card-header-title">
                    <h3>LLMs “off-the-shelf” or Pretrain-from-Scratch? Recalibrating Biases and Improving Transparency using Small-Scale Language Models</h3>
                    <h3>Learning & Human Intelligence Group</h3>
                  </div>
                  <img src="img/cst_logo.png" alt="University Logo" class="work-logo">
                </div>
                <div class="card-body">
                  <p class="card-body-title">2024</p>
                </div>
                <div class="card-body-content">
                  <p><strong>Authors:</strong> Suchir Salhan, Richard Diehl-Martinez, Zebulon Goriely, Andrew Caines, Paula Buttery</p>
                  <p>This work evaluates Small-Scale Language Models for transparency and bias reduction, comparing their performance in specialized fields against Large Language Models.</p>
                </div>
              </div>
            </div>
          </li>
          <li>
            <div class="card-base card-round-bottom">
              <div class="card-content">
                <div class="card-header">
                  <div class="card-header-title">
                    <h3>On the Potential for Maximising Minimal Means in Transformer Language Models: A Dynamical Systems Perspective</h3>
                    <h3>Cambridge Occasional Papers in Linguistics</h3>
                  </div>
                  <img src="img/linguistics_logo.png" alt="Linguistics Logo" class="work-logo">
                </div>
                <div class="card-body">
                  <p class="card-body-title">2023</p>
                </div>
                <div class="card-body-content">
                  <p><strong>Author:</strong> Suchir Salhan</p>
                  <p>Explores the application of neo-emergent linguistic models and domain-general inductive biases to Transformer-based Language Models.</p>
                </div>
              </div>
            </div>
          </li>
        </ul>
      </section>
    </main>

<ul style="list-style-type:none; padding:0; margin:0; width:100%; text-align:right;">
    <li>
        <p style="font-size:small;">
            © Suchir Salhan 2024 
        </p>
    </li>
</ul>
  </div>
</body>

</html>

