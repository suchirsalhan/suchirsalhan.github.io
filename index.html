<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Suchir Salhan</title>

  <meta name="author" content="Suchir Salhan">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="sas245.jpg">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4VJBBSSEN8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-4VJBBSSEN8');
  </script>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Suchir Salhan
                  </p>
                   <p>I am a PhD Candidate in Computer Science at the <a href="https://www.cst.cam.ac.uk/research/themes/natural-language-processing">University of Cambridge</a> (Gonville & Caius College), researching Machine Learning and Natural Language Processing and supervised by <a href="https://www.cst.cam.ac.uk/people/pjb48">Professor Paula Buttery</a>. I specialise in Machine Learning and Natural Language Processing. I am interested in developing cognitively-inspired computational systems, including alternatives to Transformer-based Large Language Models. I previously completed my Bachelor of Arts and Masters in Engineering in Computer Science and Linguistics at Gonville & Caius College, University of Cambridge, where I obtained a Starred First (Class I with Distinction) and a Distinction (also equivalent to a starred First) respectively. My research focuses on Small-Scale Language Models to improve the interpretability of Foundation Models.  
                  </p>
                  <p>
                    My research primarily is concerned with engineering more cognitively plausible Foundation Models. This is an emerging research paradigm that attempts to improve the cognitive capabilities of state-of-the-art computational systems within a cognitively plausible environment. I have interests in Machine Learning Systems, the Theory of Deep Learning and Theoretical Linguistics. My ambition is to develop data-efficient Machine Learning systems that draw on human cognition. 
                  </p>
                  <p>

                    To this end, I'm particularly interested in developing novel machine learning techniques to build scalable neural architectures that draw on formal methods (e.g., category and type theory) utilised in theoretical formalisms in Cognitive Science. 
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:sas245@cam.ac.uk">Email</a> &nbsp;/&nbsp;
                    <a href="data/cv.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/suchirsalhan/">Twitter</a> &nbsp;/&nbsp;
                    <a href="www.linkedin.com/in/ssalhan">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/suchirsalhan/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/sas245.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/sas245.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                   My research currently focuses on building small-scale Transformer-based language models. I have engineered curriculum learning (CL) strategies inspired by cutting-edge Language Acquisition frameworks. 
                  </p>
                  <p>
                    I primarily work on Transformer-based Large Language Models (LLMs), and have previously drawn on Chomskyan models of Language Acquisition. I have previously worked with Multimodal Vision-Language Models in the Language Technology Lab with Prof Nigel Collier and Fangyu Liu (now Google DeepMind).Previously, I've probed vision-language models, exploring the semantic representations of CLIP. I have worked on Nearest Neighbour Algorithms for Offline Imitation Learning (IL). I have also worked on Explainable AI and Argumentation Mining,  Shortcut Learning in Natural Language Inference                    
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td colspan="2">
                  <h3 style="padding-left: 20px;">Published work</h3>
                </td>
              </tr>
              

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/succ.png" alt="successor" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://aclanthology.org/2023.conll-babylm.10/">
                    <span class="papertitle">Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies.</span>
                  </a>
                  <br>
                  <strong>Suchir Salhan</strong>, 
                  <a href="https://www.richarddiehlmartinez.com/">Richard Diehl-Martinez</a>,
                  <a href="https://www.cst.cam.ac.uk/people/zg258">Zebulon Goriely</a>,
                  <a href="https://www.cl.cam.ac.uk/~pjb48/">Paula Buttery</a>
                  <br>
                  <em>In Preparation for CoNLL BabyLM Challenge (Paper Track)</em>, 2024;
                  <em>NeurIPS ATTRIB (Oral)</em>, 2023
                  <br>
                </td>
              </tr>

                <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/succ.png" alt="successor" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://aclanthology.org/2023.conll-babylm.10/">
                    <span class="papertitle">LLMs “off-the-shelf” or Pretrain-from-Scratch? Recalibrating Biases and Improving Transparency using Small-Scale Language Models.</span>
                  </a>
                  <br>
                  <strong>Suchir Salhan</strong>, 
                  <a href="https://www.richarddiehlmartinez.com/">Richard Diehl-Martinez</a>,
                  <a href="https://www.cst.cam.ac.uk/people/zg258">Zebulon Goriely</a>,
                  <a href="https://www.cl.cam.ac.uk/~apc38/">Andrew Caines</a>,
                  <a href="https://www.cl.cam.ac.uk/~pjb48/">Paula Buttery</a>
                  <br>
                  <em> Learning & Human Intelligence Group, Department of Computer Science & Technology </em>, 2024;
                  <br>
                </td>
              </tr>



                <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/succ.png" alt="successor" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://aclanthology.org/2023.conll-babylm.10/">
                    <span class="papertitle"> On the Potential for Maximising Minimal Means in Transformer Language Models: A Dynamical Systems Perspective. </span>
                  </a>
                  <br>
                  <strong>Suchir Salhan</strong>
                  <br>
                  <em> In Cambridge Occasional Papers in Linguistics, Department of Theoretical & Applied Linguistics </em>, 2023;
                  <br>
                  <p></p>
                  <p>
                    Computational linguists can utilise the insights of neo-emergent linguistic models, an approach to grammar construction that relies heavily on domain-general inductive biases, to address extant challenges associated with the syntactic and typological capabilities of state-of-the-art Transformer-based Language Models (LMs), which underpin systems like Google Translate and ChatGPT. I offer a synthesis of the inductive biases of Transformer-based LMs that are reminiscent of two properties emphasised in a neo-emergent model called ‘Maximise Minimal Means’ (MMM) (Biberauer 2011: et seq). Subsequently, I undertake an analysis of the structural generalisation capabilities of Transformer-based LMs through a creative probing case study of subject-verb agreement, which indicates that these models are unable to perform the crucial NO > ALL > SOME learning dynamic associated with MMM. In light of these empirical findings, I offer a theoretical argument about how MMM and associated Dynamical Systems Theory (Bosch 2022, 2023) can be viewed as a linguistically motivated goal– as proposed by Emerson (2020b) – for Transformer LMs. I propose that the predictions of this neo-emergentist approach translate into theoretical principles and practical recommendations to improve the syntactic capabilities of Transformer-based LMs in a typologically consistent manner. This perspective can stimulate a productive interdisciplinary discussion on how linguistic theory can help engineer LMs with better syntactic and typological capabilities.
                
                  </p>
                  
                </td>
              </tr>

              <tr>
                <td colspan="2">
                  <h3 style="padding-left: 20px;">Computational Projects</h3>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/personality.jpg" alt="attention heads" width="160">
                </td>
                <td width="75%" valign="middle">
                  <!-- <a href="https://example.com"> -->
                  <a href="https://www.cam.ac.uk/research/news/claims-ai-can-boost-workplace-diversity-are-spurious-and-dangerous-researchers-argue">
                    <span class="papertitle"> Argumentation Mining </span>
                  </a>
                  <br>
                  <strong>Suchir Salhan</strong>, 
                  <br>
                  <em> Department of Computer Science and Technology Natural Language Processing UROP </em>, 2020
                  <br>
                  <!--<a href="https://www.cam.ac.uk/research/news/claims-ai-can-boost-workplace-diversity-are-spurious-and-dangerous-researchers-argue">project </a>
                  /
                  <a href="https://example.com">arXiv</a>
                  /
                  <a href="https://example.com">reviews</a>
                  /
                  <a href="https://example.com">bibTeX</a>-->
                  <p> I was offered a UROP research project by my Director of Studies Prof Paula Buttery and Dr Andrew Caines. I worked under the supervision of computational linguists from the Automated Language Teaching and Assessment (ALTA) group in the Department of Computer Science and Technology. I was the only first-year student admitted to the two-month UROP programme. I worked on a project in collaboration with Thiemo Wambsganss on developing the back-end machine learning architecture for an application that supports the argumentation skills of English language learners. I trained and evaluated the performance of state-of-the-art transformer language models on downstream argumentation mining tasks, began working on the deployment of the pre-trained model in the application, and submitted the ethics application for the experimental evaluation of the educational outcomes of the application. I presented my work to members of the ALTA group and the project sponsors from Cambridge Assessment who funded my UROP project. I also had the opportunity to attend machine learning classes, and seminars on Dialogue Systems, Ethics in NLP, active learning paradigms and educational technology. 
                  </p>
                </td>
              </tr>
              
  
  

              
            </tbody>
          </table>
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Theoretical Linguistics and Cognitive Science</h2>
                  <p>  I have worked as a Research Assistant for a small project on corpus-based studies of code-switching with Dr Li Nguyen.                      
                  </p>
                  <p>  My theoretical  linguistics interests work against the background assumptions of neo-emergentist approaches, which assume a minimally endowed (genetic) component of the grammar and situate the burden of acquisition to the learner. Within this approach, I am concerned by questions of Learnability and look to formalise that draw on Dynamical Systems Theory. I am keen to explore the potential relevance of information-theoretic approaches to acquisition and to explore syntax-phonology interface phenomena. 
                  </p>
                </td>
              </tr>
            </tbody>
          </table>




  
          
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    © Suchir Salhan 2024 
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
