---
title: Home
layout: default
background: '/img/bg-index.jpg'
---

<!-- Home Intro

<! If need to update:
--h2 style="text-align: center;" >ðŸš§ Site Currently Under Construction ðŸš§</h2>
<h2 style="text-align: center;">... Please visit again soon ...</h2-->

================================================== -->
{% if page.url == "/" %} 

  <div class="row align-items-center justify-content-between">
    <div class="col-md-6 text-right pl-0 pl-lg-4">
      <img class="intro" height="500" src="{{site.baseurl}}/sas245-photo.png">      
    </div>
    <div class="col-md-6">
      <h1 class="font-weight-bold mb-4 serif-font">Suchir Salhan</h1>
      <p >Suchir Salhan is a Computer Science PhD candidate at the University of Cambridge, working on <b>Small Language Models</b> under the supervision of  <u><a href ="https://www.cl.cam.ac.uk/~pjb48/">Prof Paula Buttery</a></u>.  
      and the <b> Founder of <u><a href="https://www.per-capita.co.uk">Per Capita Media</a></u>, Cambridge University's newest independent media organisation</b>. To contact me, email <a href="mailto:sas245@cam.ac.uk" rel="noopener noreferrer">sas245@cam.ac.uk</a> 
   </p>
      <p><br></p>
  </div>
</div>
{% endif %}

<div class="rounded mb-5 hero">
  <div class="row align-items-center justify-content-between">
      <div class="col-md-6 text-right pl-0 pl-lg-4">
            <h3 class="font-weight-bold mb-4 serif-font">Small Language Models:</h3>
    Modern Natural Language Processing (NLP) is dominated by large pre-trained, highly parameterised neural networks trained on extremely large web-mined corpora. Training and inference using such models are incredibly costly, and the benefits of the pre-train/fine-tune paradigm are unclear for domain-specific downstream tasks. Recent advances in language modeling consist in pretraining highly parameterized neural networks on extremely large web-mined text corpora. Training and inference with such models can be costly in practice, which incentivizes the use of smaller counterparts. Additionally, theoretical linguists and cognitive scientists have highlighted several weaknesses with state-of-the-art foundation models.
    <div class="d-flex justify-content-center gap-3">
      <a href="{{site.baseurl}}/slm" class="btn btn-dark text-white px-5 btn-lg"> Cambridge Small Language Models</a>
    </div>
  </div>
  <div class="col-md-6">
    <h1 class="font-weight-bold mb-4 serif-font">Handouts & Resources</h1>
</div>
</div>
 </div>





<div class="card-body-title">
  <div class="col-md-6">
    <h3 class="font-weight-bold mb-4 serif-font">Recent News:</h3>
  </div>
</div>

<section class="row">
{% assign i = 0 %}  
  {% for post in site.posts %}
      {% if post.featured == false %}
        {% if i <3  %}
            <div class="col-md-4 mb-5">
            {% include postbox.html %}
            </div>
            {% assign i = i | plus:1 %}
        {% endif %}
      {% endif %}
  {% endfor %}
  </div>
</section>



{% for thing in things %}
    {% assign i = i | plus:1 %}
{% endfor %}

<!-- Posts List with Sidebar (except featured)
================================================== -->
<h3 class="font-weight-bold mb-4 serif-font"><a href = '/publications/'>Publications:</a></h3>
<br>
<section class="row">
  <div class="col-sm-8">
    <div class="row">

      {% unless post.featured == false %}

      {% assign i = 0 %}  
      {% for post in site.posts %}
      {% if post.featured == true %}
        {% if i < 4  %}
        <div class="col-md-6 mb-5">
          {% include postbox.html %}
        </div>
            {% assign i = i | plus:1 %}
        {% endif %}
      {% endif %}
  {% endfor %}


      {% endunless %}
    
    </div>
    
    <!-- Pagination -->
    <div class="bottompagination">
      <span class="navigation" role="navigation">
          {% include pagination.html %}
      </span>
    </div>
  </div>
  <div class="col-sm-4">
    {% include sidebar.html %}
  </div>
</section>

<h3 class="font-weight-bold mb-4 serif-font">Blog:</h3>
<br>

