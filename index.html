<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Suchir Salhan</title>

  <meta name="author" content="Suchir Salhan">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="sas245.jpg">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4VJBBSSEN8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-4VJBBSSEN8');
  </script>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Suchir Salhan
                  </p>
                   <p>I am a PhD Candidate in Computer Science at the <a href="https://www.cst.cam.ac.uk/research/themes/natural-language-processing">University of Cambridge</a>, researching Machine Learning and Natural Language Processing. I have a “Starred First” in my Bachelor of Arts and Distinction in my MEng in Computer Science and Linguistics from the University of Cambridge. My research focuses on Small-Scale Language Models to improve the interpretability of Foundation Models.  
                  </p>
                  <p>
                    I have interests in Machine Learning Systems, the Theory of Deep Learning and Theoretical Linguistics. My ambition is to develop data-efficient Machine Learning systems that draw on human cognition. 
                  </p>
                  <p>

                    To this end, I'm particularly interested in developing novel machine learning techniques to build scalable neural architectures that draw on formal methods (e.g., category and type theory) utilised in theoretical formalisms in Cognitive Science. 
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:sas245@cam.ac.uk">Email</a> &nbsp;/&nbsp;
                    <a href="data/cv.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/suchirsalhan/">Twitter</a> &nbsp;/&nbsp;
                    <a href="www.linkedin.com/in/ssalhan">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/suchirsalhan/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/sas245.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/sas245.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                   My research currently focuses on building small-scale Transformer-based language models. I have engineered curriculum learning (CL) strategies inspired by cutting-edge Language Acquisition frameworks. 
                  </p>
                  <p>
                    Previously, I've probed vision-language models, exploring the semantic representations of CLIP. I have worked on Nearest Neighbour Algorithms for Offline Imitation Learning (IL). I have also worked on Explainable AI and Argumentation Mining,  Shortcut Learning in Natural Language Inference                    
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td colspan="2">
                  <h3 style="padding-left: 20px;">Published work</h3>
                </td>
              </tr>
              

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/succ.png" alt="successor" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2312.09230.pdf">
                    <span class="papertitle">Successor Heads: Recurring, Interpretable Attention Heads In The Wild</span>
                  </a>
                  <br>
                  <a href="https://r-gould.github.io/">Rhys Gould</a>, 
                  <strong>Euan Ong</strong>, 
                  <a href="https://go281.user.srcf.net/">George Ogden</a>,
                  <a href="https://arthurconmy.github.io/about/">Arthur Conmy</a>
                  <br>
                  <em>ICLR</em>, 2024;
                  <em>NeurIPS ATTRIB (Oral)</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2312.09230">arXiv</a>
                  /
                  <a href="https://openreview.net/forum?id=kvcbV8KQsi">reviews</a>
                  /
                  <a href="https://successor-heads.notion.site/">project page</a>
                  /
                  <a href="https://x.com/rgould0/status/1737261015245451399">tweeprint</a>
                  <p>We discovered <em>successor heads</em>: attention heads present in a range of LLMs that increment tokens from ordinal sequences (e.g. numbers, months and days). We isolated a common numeric subspace within embedding space, that for any given token (e.g. 'February'), encodes the index of that token within its ordinal sequence (e.g. months). We also found that numeric token representations can be decomposed into interpretable features representing the value of the token <em>mod 10</em>, which can be used to edit the numeric value of the representation via vector arithmetic.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/hijacks.png" alt="hijacks" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2309.00236.pdf">
                    <span class="papertitle">Image Hijacks: Adversarial Images can Control Generative Models at
                      Runtime</span>
                  </a>
                  <br>
                  <a href="https://petar-v.com/">Luke Bailey</a>*, <strong>Euan Ong*</strong>, <a
                    href="http://people.eecs.berkeley.edu/~russell/">Stuart Russell</a>, <a
                    href="http://scottemmons.com/">Scott Emmons</a> (* denotes equal contribution)
                  <br>
                  <em>ICML</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2309.00236">arXiv</a>
                  /
                  <a href="https://image-hijacks.github.io/">project page + demo</a>
                  /
                  <a href="https://twitter.com/emmons_scott/status/1704166628089565262">tweeprint</a>
                  <p>We discovered that adversarial images can hijack the behaviour of vision-language models (VLMs) at
                    runtime. We developed a general method for crafting these <i>image hijacks</i>,
                    and trained image hijacks forcing VLMs to output arbitrary text, leak their context window and
                    comply with harmful instructions. We also derived an algorithm to train hijacks forcing 
                    VLMs to behave as though they were given an arbitrary prompt, which we used to make them believe the Eiffel Tower is in Rome.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/lcm.png" alt="monoids" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://proceedings.mlr.press/v198/ong22a/ong22a.pdf">
                    <span class="papertitle">Learnable Commutative Monoids for Graph Neural Networks</span>
                  </a>
                  <br>
                  <strong>Euan Ong</strong>, <a href="https://petar-v.com/">Petar Veličković</a>
                  <br>
                  <em>Learning on Graphs Conference</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2212.08541">arXiv</a>
                  /
                  <a href="https://openreview.net/forum?id=WtFobB28VDey">reviews</a>
                  /
                  <a href="https://monoids.notion.site/">project page</a>
                  /
                  <a href="https://twitter.com/PetarV_93/status/1602298299083628544">tweeprint</a>
                  <p>Using ideas from abstract algebra and functional programming, we built a new GNN aggregator that
                    beats the state of the art on complex aggregation problems (especially out-of-distribution), while
                    remaining efficient and parallelisable on large graphs.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2">
                  <h3 style="padding-left: 20px;">Other projects</h3>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/personality.jpg" alt="attention heads" width="160">
                </td>
                <td width="75%" valign="middle">
                  <!-- <a href="https://example.com"> -->
                  <a href="https://www.cam.ac.uk/research/news/claims-ai-can-boost-workplace-diversity-are-spurious-and-dangerous-researchers-argue">
                    <span class="papertitle">Personality Machine</span>
                  </a>
                  <br>
                  <strong>Euan Ong</strong>*, 
                  <a href="https://www.linkedin.com/in/jamie-chen-a186161b4/">Jamie Chen</a>*,
                  <a href="https://www.linkedin.com/in/kyra-zhou/">Kyra Zhou</a>*,
                  <a href="https://www.linkedin.com/in/marcus-handley-a2a6b1179/">Marcus Handley</a>*,
                  <a href="https://www.linkedin.com/in/mingle-chen-8b4986181/">Mingle Chen</a>*,
                  <a href="https://www.linkedin.com/in/ori-vasilescu-b973a722a/">Ori Vasilescu</a>*,
                  <a href="https://www.eleanordrage.com/">Eleanor Drage</a>
                  (* denotes equal contribution)
                  <br>
                  <em>CST Group Project</em>, 2022
                  <br>
                  <!--<a href="https://www.cam.ac.uk/research/news/claims-ai-can-boost-workplace-diversity-are-spurious-and-dangerous-researchers-argue">project </a>
                  /
                  <a href="https://example.com">arXiv</a>
                  /
                  <a href="https://example.com">reviews</a>
                  /
                  <a href="https://example.com">bibTeX</a>-->
                  <p>In collaboration with the Centre for Gender Studies at Cambridge, we built a tool highlighting the questionable logic behind the use of AI-driven personality assessments often used in hiring. Our tool demonstrates how arbitrary changes in facial expression, clothing, lighting and background can give radically different personality readings, and was featured in <a href="https://www.bbc.co.uk/news/technology-63228466">the BBC</a> and <a href="https://www.telegraph.co.uk/news/2022/10/10/why-shouldnt-wear-glasses-interview-robot/">the Telegraph</a>.
                  </p>
                </td>
              </tr>
              
  
  

              
            </tbody>
          </table>
          <!-- 
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Community</h2>
                </td>
              </tr>
            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Miscellanea</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
                <td width="75%" valign="center">
                  <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                  <br>
                  <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                  <br>
                  <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member,
                    CVPR 2021</a>
                  <br>
                  <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                  <br>
                  <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cs188.jpg" alt="cs188">
                </td>
                <td width="75%" valign="center">
                  <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor,
                    CS188 Spring 2011</a>
                  <br>
                  <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor,
                    CS188 Fall 2010</a>
                  <br>
                  <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd
                    Edition</a>
                </td>
              </tr>


              <tr>
                <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                  <h2>Basically <br> Blog Posts</h2>
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                  <br>
                  <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain
                    Functions</a>
                  <br>
                  <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                  <br>
                  <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How
                    Can Academics Adapt?</a>
                </td>
              </tr>


            </tbody>
          </table> -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    © Suchir Salhan 2024 
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
