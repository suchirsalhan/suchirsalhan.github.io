<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Suchir Salhan</title>
  <meta name="author" content="Suchir Salhan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="sas245.jpg">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4VJBBSSEN8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-4VJBBSSEN8');
  </script>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:100%;vertical-align:middle">
                  <p class="name" style="text-align: center;">Suchir Salhan</p>
                  <p style="text-align:center">
                    <a href="images/sas245.jpg"><img style="width:50%;max-width:50%" alt="profile photo" src="images/sas245.jpg" class="hoverZoomLink"></a>
                  </p>
                  <p>I am a PhD Candidate in Computer Science at the <a href="https://www.cst.cam.ac.uk/research/themes/natural-language-processing">University of Cambridge</a> (Gonville & Caius College), researching Machine Learning and Natural Language Processing and supervised by <a href="https://www.cst.cam.ac.uk/people/pjb48">Professor Paula Buttery</a>. I specialise in Machine Learning and Natural Language Processing. I am interested in developing cognitively-inspired computational systems, including alternatives to Transformer-based Large Language Models. I previously completed my Bachelor of Arts and Masters in Engineering in Computer Science and Linguistics at Gonville & Caius College, University of Cambridge, where I obtained a Starred First (Class I with Distinction) and a Distinction (also equivalent to a starred First) respectively. My research focuses on Small-Scale Language Models to improve the interpretability of Foundation Models.  
                  </p>
                  <p>
                    My research primarily is concerned with engineering more cognitively plausible Foundation Models. This is an emerging research paradigm that attempts to improve the cognitive capabilities of state-of-the-art computational systems within a cognitively plausible environment. I have interests in Machine Learning Systems, the Theory of Deep Learning and Theoretical Linguistics. My ambition is to develop data-efficient Machine Learning systems that draw on human cognition. 
                  </p>
                  <p>
                    To this end, I'm particularly interested in developing novel machine learning techniques to build scalable neural architectures that draw on formal methods (e.g., category and type theory) utilised in theoretical formalisms in Cognitive Science. 
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:sas245@cam.ac.uk">Email</a> &nbsp;/&nbsp;
                    <a href="data/cv.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/suchirsalhan/">Twitter</a> &nbsp;/&nbsp;
                    <a href="www.linkedin.com/in/ssalhan">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/suchirsalhan/">Github</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    My research currently focuses on building small-scale Transformer-based language models. I have engineered curriculum learning (CL) strategies inspired by cutting-edge Language Acquisition frameworks.
                  </p>
                  <p>
                    I primarily work on Transformer-based Large Language Models (LLMs), and have previously drawn on Chomskyan models of Language Acquisition. I have previously worked with Multimodal Vision-Language Models in the Language Technology Lab with Prof Nigel Collier and Fangyu Liu (now Google DeepMind). Previously, I've probed vision-language models, exploring the semantic representations of CLIP. I have worked on Nearest Neighbour Algorithms for Offline Imitation Learning (IL). I have also worked on Explainable AI and Argumentation Mining, Shortcut Learning in Natural Language Inference.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td colspan="2">
                  <h3 style="padding-left: 20px;">Published work</h3>
                </td>
              </tr>
              <tr>
                <td width="100%" valign="middle">
                  <a href="https://aclanthology.org/2023.conll-babylm.10/">
                    <span class="papertitle">Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies.</span>
                  </a>
                  <br>
                  <strong>Suchir Salhan</strong>, 
                  <a href="https://www.richarddiehlmartinez.com/">Richard Diehl-Martinez</a>,
                  <a href="https://www.cst.cam.ac.uk/people/zg258">Zebulon Goriely</a>,
                  <a href="https://www.cl.cam.ac.uk/~pjb48/">Paula Buttery</a>
                  <br>
                  <em>In Preparation for CoNLL BabyLM Challenge (Paper Track)</em>, 2024
                  <br>
                </td>
              </tr>
              <tr>
                <td width="100%" valign="middle">
                  <a href="https://aclanthology.org/2023.conll-babylm.10/">
                    <span class="papertitle">LLMs “off-the-shelf” or Pretrain-from-Scratch? Recalibrating Biases and Improving Transparency using Small-Scale Language Models.</span>
                  </a>
                  <br>
                  <strong>Suchir Salhan</strong>, 
                  <a href="https://www.richarddiehlmartinez.com/">Richard Diehl-Martinez</a>,
                  <a href="https://www.cst.cam.ac.uk/people/zg258">Zebulon Goriely</a>,
                  <a href="https://www.cl.cam.ac.uk/~apc38/">Andrew Caines</a>,
                  <a href="https://www.cl.cam.ac.uk/~pjb48/">Paula Buttery</a>
                  <br>
                  <em> Learning & Human Intelligence Group, Department of Computer Science & Technology </em>, 2024
                  <br>
                  <p></p>
                  <p>
                    Our work has found Small-Scale Language Models (SSLMs) perform competitively against certain benchmarks for LLM Evaluation cross-linguistically. SSLMs have improved transparency and can outperform LLMs in specialist domains, demanding a more judicious assessment of the benefits of “pretraining-from-scratch” compared to using LLMs “off-the-shelf”.
                  </p>
                </td>
              </tr>
              <tr>
                <td width="100%" valign="middle">
                  <a href="https://aclanthology.org/2023.conll-babylm.10/">
                    <span class="papertitle"> On the Potential for Maximising Minimal Means in Transformer Language Models: A Dynamical Systems Perspective. </span>
                  </a>
                  <br>
                  <strong>Suchir Salhan</strong>
                  <br>
                  <em> In Cambridge Occasional Papers in Linguistics, Department of Theoretical & Applied Linguistics </em>, 2023;
                  <br>
                  <p></p>
                  <p>
                    Computational linguists can utilise the insights of neo-emergent linguistic models, an approach to grammar construction that relies heavily on domain-general inductive biases, to address extant challenges associated with the syntactic and typological capabilities of state-of-the-art Transformer-based Language Models (LMs), which underpin systems like Google Translate and ChatGPT. I offer a synthesis of the inductive biases of Transformer-based LMs that are reminiscent of Dynamical Systems Theory (DST) approaches in human cognition. In doing so, I put forward a research agenda that will strengthen the case for minimalism in deep learning.
                  </p>
                </td>
              </tr>
              <tr>
                <td colspan="2">
                  <h2 style="padding-left: 20px;">Recent Teaching</h2>
                </td>
              </tr>
              <tr>
                <td width="100%" valign="middle">
                  <p></p>
                  <span class="papertitle">Supervision in MPhil Advanced NLP (2023-2024):</span>
                  <br>
                  <strong>University of Cambridge</strong>, Department of Computer Science and Technology (Language Technology Lab)
                  <br>
                  <p></p>
                  <p>
                    I supervise MPhil students in Natural Language Processing, focusing on novel deep learning techniques for language representation and acquisition. My supervisions cover diverse topics, including curriculum learning strategies, language model interpretability, and dynamic neural architecture search.
                  </p>
                  <p></p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>
