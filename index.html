---
title: Home
layout: default
background: '/img/bg-index.jpg'
---

<!-- Home Intro

<! If need to update:
--h2 style="text-align: center;" >ðŸš§ Site Currently Under Construction ðŸš§</h2>
<h2 style="text-align: center;">... Please visit again soon ...</h2-->

================================================== -->
{% if page.url == "/" %} 

<br>
<br>
  <h1 class="font-weight-bold mb-4">Suchir Salhan</h1>
  <div class="row align-items-center justify-content-between">
    <div class="col-md-6 text-left pl-0 pl-lg-4">
      <img class="intro" height="500" src="{{site.baseurl}}/sas245-photo.png">      
    </div>
    <div class="col-md-6">
      <h3 class="font-weight-bold mb-4 article-post serif-font">Suchir Salhan is a Computer Science PhD candidate at the University of Cambridge, working on <b>Small Language Models</b> under the supervision of  <u><a href ="https://www.cl.cam.ac.uk/~pjb48/">Prof Paula Buttery</a></u>  
      and the <b> Founder of <u><a href="https://www.per-capita.co.uk">Per Capita Media</a></u>, Cambridge University's newest independent media organisation</b>.</a> 
   </h3>
      <p><br></p>
  </div>
</div>
{% endif %}

<br>
<be>


<br>
<br>

  <div class="card-body-title">
  <div class="col-md-6">
    <h3 class="font-weight-bold mb-4 serif-font">Cambridge Small Language Models: Cognitively-Inspired Alternatives to Transformer-based LLMs</h3>
  </div>
</div>

  <p class="article-post serif-font">  My research is concerned with building more scalable small Language Models. While industry-led efforts have built competitive NLP systems that have fundamentally shifted the job of the (academic) NLP researcher in various ways, contemporary Natural Language Processing (NLP) still has several fundamental, open questions to address that are not obviously ancillary to commercial AI research labs. </p>

  
  <p class="article-post serif-font">   

<b> Architecture & Computational Complexity:</b> The viability of 'Small LMs' as a coherent research programme relies on a successful consideration of efficiency, acceleration and architectural questions. There is a growing recognition that the computational complexity of self-attention in Transformers is suboptimal in various respects. 

<br>

<b> Cognitively-inspired AI:</b> The emergent capabilities of Transformers are subject to a great deal of interpretability work, however there is a clear mismatch between human language acquisition (which is data-efficient in many regards) and the data-hungriness of Transformers. I am personally very invested in research questions that draw on insights from language acquisition to guide architectural alternatives to 'vanilla' Transformers. 

<br>

<b> Training Dynamics, Evaluation and Scalability:</b> Benchmarking is a fundamental part of guiding contemporary AI systems, and requires an inherently interdisciplinary approach to be meaningful.  Leading metrics are often variously incomplete or inadequate. However, equally, we should not only be interested reporting overall scores and metrics â€“ training dynamics of models are equally important.

<br>

<b> Domain-Specificity:</b> Practioners interested in domain-specific Machine Learning (e.g., in educational, legal, biomedical domains) do not have sufficient control over the capabilities of Language Models. While novel post-training techniques are addressing this in various ways, techniques and strategies related to pretraining are equally important. The ethical and societal importance of domain-specific Language Models are huge. It is vitally important that research into the control over systems are not relegated solely to industry and Big Tech â€“ who are guide by a different set of commercial prioritiesâ€“ particularly for research areas associated with large human costs (in labour and social terms). 
  </p>
  
<div class="rounded mb-5 hero">
  <div class="row align-items-center justify-content-between">
      <div class="col-md-6 text-left pl-0 pl-lg-4">
              <p class="article-post serif-font">     Modern Natural Language Processing (NLP) is dominated by large pre-trained, highly parameterised neural networks trained on extremely large web-mined corpora. Training and inference using such models are incredibly costly, and the benefits of the pre-train/fine-tune paradigm are unclear for domain-specific downstream tasks. Recent advances in language modelling rely on pretraining highly parameterized neural networks on extremely large web-mined text corpora. Training and inference with such models can be costly in practice, which incentivises the use of smaller counterparts. Additionally, theoretical linguists and cognitive scientists have highlighted several weaknesses with state-of-the-art foundation models.
 </p>
    <div class="d-flex justify-content-center gap-3">
    </div>
  </div>
  <div class="col-md-6">
          <a href="{{site.baseurl}}/slm" class="btn btn-dark text-white px-5 btn-lg"> Cambridge Small Language Models</a>

      <p class="article-post serif-font"> This technical 'blog' on Small LMs discusses interesting techniques and perspectives from collaborators and other Machine Learning, NLP and Cognitive Science researchers. If this is something that may be of interest, please consider getting in touch!  To contact me, email <a href="mailto:sas245@cam.ac.uk" rel="noopener noreferrer">sas245@cam.ac.uk. </p>
</div>
</div>
 </div>

  
<div class="card-body-title">
  <div class="col-md-6">
    <h3 class="font-weight-bold mb-4 serif-font">Recent News:</h3>
  </div>
</div>

  <p class="article-post serif-font">   I organise the Natural Language & Information Processing (NLIP) Seminars in the Department of Computer Science & Technology, University of Cambridge. </p>


<section class="row">
{% assign i = 0 %}  
  {% for post in site.posts %}
      {% if post.featured == false %}
        {% if i <3  %}
            <div class="col-md-4 mb-5">
            {% include postbox.html %}
            </div>
            {% assign i = i | plus:1 %}
        {% endif %}
      {% endif %}
  {% endfor %}
  </div>
</section>


  
<div class="card-body-title">
  <div class="col-md-6">
    <h3 class="font-weight-bold mb-4 serif-font">Handouts & Teaching Materials:</h3>
  </div>
</div>


    <p class="article-post serif-font">   I delivered my first guest lecture in November 2024 for an MPhil course in the University of Cambridge with Prof Buttery and Dr Fermin Moscoco del Prado Martin on Language Model Evaluation. In Lent 2025, I am the Teaching Assistant (a new role equivalent to Lead Demonstrator) for CST IA Machine Learning & Real World Data and am a supervisor for Machine Learning & Bayesian Inference (MBLI) [CST Part II]. I am supervising an MPhil Project on Language Model Stability. </p>


{% for thing in things %}
    {% assign i = i | plus:1 %}
{% endfor %}

<!-- Posts List with Sidebar (except featured)
================================================== -->
<h3 class="font-weight-bold mb-4 serif-font"><a href = '/publications/'>Publications:</a></h3>
<br>
<section class="row">
  <div class="col-sm-8">
    <div class="row">

      {% unless post.featured == false %}

      {% assign i = 0 %}  
      {% for post in site.posts %}
      {% if post.featured == true %}
        {% if i < 4  %}
        <div class="col-md-6 mb-5">
          {% include postbox.html %}
        </div>
            {% assign i = i | plus:1 %}
        {% endif %}
      {% endif %}
  {% endfor %}


      {% endunless %}
    
    </div>
    
    <!-- Pagination -->
    <div class="bottompagination">
      <span class="navigation" role="navigation">
          {% include pagination.html %}
      </span>
    </div>
  </div>
  <div class="col-sm-4">
    {% include sidebar.html %}
  </div>
</section>

<br>

