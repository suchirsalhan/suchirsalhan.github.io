<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Suchir Salhan</title>
  <meta name="author" content="Suchir Salhan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/style.css" rel="stylesheet">
  <link href="css/font.css" rel="stylesheet">  
  <link rel="icon" href="sas245.jpg">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4VJBBSSEN8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-4VJBBSSEN8');
  </script>
</head>

<body style="background-color: #f0f0f0;">
  <div class="center">
    <header>
      <h1>Suchir Salhan</h1>
      <nav class="navbar">
        <a class="selected-navbar">Home</a>
        <a href="https://suchirsalhan.github.io/slm" target="_blank">Cambridge Small LMs</a> 
      </nav>
    </header>

    <img src="images/sas245.jpg" alt="Suchir Salhan">

    <div class="spacer"></div>
    
    <h1 style="margin-top: 0; font-size: 2.5rem">
      Computer Science PhD at <br class="ignore-on-mobile"/> University of Cambridge
    </h1>
    <img src="images/computer-lab.jpg" alt="Picture of the William Gates Building." class="hero">

    <div class="spacer"></div>
    <main>
      <section class="group-section">
        <h2 >Academic Interests</h2>
        <ul>
          <li >Natural Language Processing</li>
          <li>Machine Learning</li>
        </ul>
        <p><a href="https://suchirsalhan.github.io/slm" target="_blank"> Learn more about Cambridge Small Language Models</a></p> <!-- New link added here -->
      </section>

      <section>
        <h2>Publications</h2>
        <ul>
          <li>
            <div class="card-base card-round-top">
              <div class="card-content">
                <div class="card-header">
                  <div class="card-header-title">
                    <h3>Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies</h3>
                    <h3>CoNLL BabyLM Challenge</h3>
                  </div>
                  <img src="img/conll_logo.png" alt="CoNLL Logo" class="work-logo" style="width: 8rem">
                </div>
                <div class="card-body">
                  <p class="card-body-title">2024</p>
                </div>
                <div class="card-body-content">
                  <p><strong>Authors:</strong> Suchir Salhan, Richard Diehl-Martinez, Zebulon Goriely, Paula Buttery</p>
                  <p>This publication introduces Small-Scale Language Models (SSLMs) trained on developmentally plausible corpora. It explores cognitive curriculum learning strategies that provide insights into syntactic learning across different languages.</p>
                </div>
              </div>
            </div>
          </li>
          <li>
            <div class="card-base">
              <div class="card-content">
                <div class="card-header">
                  <div class="card-header-title">
                    <h3>LLMs “off-the-shelf” or Pretrain-from-Scratch? Recalibrating Biases and Improving Transparency using Small-Scale Language Models</h3>
                    <h3>Learning & Human Intelligence Group</h3>
                  </div>
                  <img src="img/cst_logo.png" alt="University Logo" class="work-logo">
                </div>
                <div class="card-body">
                  <p class="card-body-title">2024</p>
                </div>
                <div class="card-body-content">
                  <p><strong>Authors:</strong> Suchir Salhan, Richard Diehl-Martinez, Zebulon Goriely, Andrew Caines, Paula Buttery</p>
                  <p>This work evaluates Small-Scale Language Models for transparency and bias reduction, comparing their performance in specialized fields against Large Language Models.</p>
                </div>
              </div>
            </div>
          </li>
          <li>
            <div class="card-base card-round-bottom">
              <div class="card-content">
                <div class="card-header">
                  <div class="card-header-title">
                    <h3>On the Potential for Maximising Minimal Means in Transformer Language Models: A Dynamical Systems Perspective</h3>
                    <h3>Cambridge Occasional Papers in Linguistics</h3>
                  </div>
                  <img src="img/linguistics_logo.png" alt="Linguistics Logo" class="work-logo">
                </div>
                <div class="card-body">
                  <p class="card-body-title">2023</p>
                </div>
                <div class="card-body-content">
                  <p><strong>Author:</strong> Suchir Salhan</p>
                  <p>Explores the application of neo-emergent linguistic models and domain-general inductive biases to Transformer-based Language Models.</p>
                </div>
              </div>
            </div>
          </li>
        </ul>
      </section>
    </main>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
              © Suchir Salhan 2024 
            </p>
          </td>
        </tr>
      </tbody>
    </table>
  </div>
</body>

</html>

