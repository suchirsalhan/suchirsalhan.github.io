---
title: Home
layout: default
background: '/img/bg-index.jpg'
---
================================================== -->
{% if page.url == "/" %} 

<br>
<br>
  <h1 class="font-weight-bold mb-4">Suchir Salhan</h1>
  <div class="row align-items-center justify-content-between">
    <div class="col-md-6 text-left pl-0 pl-lg-4">
      <img class="intro" height="500" src="{{site.baseurl}}/sas245-photo.png">      
    </div>
    <div class="col-md-6">
      <h3 class="font-weight-bold mb-4 article-post serif-font">Suchir Salhan is a Computer Science PhD candidate at the University of Cambridge, working on <b>Small Language Models</b> under the supervision of  <u><a href ="https://www.cl.cam.ac.uk/~pjb48/">Prof Paula Buttery</a></u>  
      and the <b> Founder of <u><a href="https://www.per-capita.co.uk">Per Capita Media</a></u>, Cambridge University's newest independent media organisation</b>.</a> 
      </h3>
      <p> To contact me, email <u><a href="mailto:sas245@cam.ac.uk" rel="noopener noreferrer">sas245@cam.ac.uk. </a></u></p>
           <p><b> Quick Links: </b></p>
          <ul>
          <li><a href="{{site.baseurl}}/tripos">Cambridge Tripos Notes (Computer Science & Linguistics)</a></li>
          <li><a href="{{site.baseurl}}/slm> Small Language Models</a></li>
          </ul>
    </div>
</div>

{% endif %}

<br>
<be>


<br>
<br>

    <h3 class="font-weight-bold mb-4 serif-font">Cambridge Small Language Models: Cognitively-Inspired Alternatives to Transformer-based LLMs</h3>


  <p class="article-post serif-font">  My research is concerned with building scalable small Language Models. While industry-led efforts have built competitive LLMs that have fundamentally shifted the job of the contemporary Natural Language Processing (academic) researcher in various ways, there are still several fundamental, open questions to address that are not obviously ancillary to commercial AI research labs. </p>

  <p class="article-post serif-font">     Modern Natural Language Processing (NLP) is dominated by large pre-trained, highly parameterised neural networks trained on extremely large web-mined corpora. Training and inference using such models are incredibly costly, and the benefits of the pre-train/fine-tune paradigm are unclear for domain-specific downstream tasks. Recent advances in language modelling rely on pretraining highly parameterized neural networks on extremely large web-mined text corpora. Training and inference with such models can be costly in practice, which incentivises the use of smaller counterparts. Additionally, theoretical linguists and cognitive scientists have highlighted several weaknesses with state-of-the-art foundation models.


  <div class="rounded mb-5 hero">
  <div class="row align-items-center justify-content-between">
     <a href="{{site.baseurl}}/slm" class="btn btn-dark text-white px-5 btn-lg"> </a>
  <p class="article-post serif-font"> The <u><a href={{site.baseurl}}/slm">Cambridge Small Language Models</a></u> technical 'blog' focuses on interesting techniques and perspectives from collaborators and other Machine Learning, NLP and Cognitive Science researchers that broadly relate to the above issues.</p>
  </div>
  </div>


  
      <p class="article-post serif-font">   
<b> Architecture & Computational Complexity:</b> The viability of 'Small LMs' as a coherent research programme relies on a successful consideration of efficiency, acceleration and architectural questions. There is a growing recognition that the computational complexity of self-attention in Transformers is suboptimal in various respects. 

<br>
<br>


<b> Cognitively-inspired AI:</b> The emergent capabilities of Transformers are subject to a great deal of interpretability work, however there is a clear mismatch between human language acquisition (which is data-efficient in many regards) and the data-hungriness of Transformers. I am personally very invested in research questions that draw on insights from language acquisition to guide architectural alternatives to 'vanilla' Transformers. 

<br>
<br>

<b> Training Dynamics, Evaluation and Scalability:</b> Benchmarking is a fundamental part of guiding contemporary AI systems, and requires an inherently interdisciplinary approach to be meaningful.  Leading metrics are often variously incomplete or inadequate. However, equally, we should not only be interested reporting overall scores and metrics – training dynamics of models are equally important.

<br>
<br>

<b> Domain-Specificity:</b> Practioners interested in domain-specific Machine Learning (e.g., in educational, legal, biomedical domains) do not have sufficient control over the capabilities of Language Models. While novel post-training techniques are addressing this in various ways, techniques and strategies related to pretraining are equally important. The ethical and societal importance of domain-specific Language Models are huge. It is vitally important that research into the control over systems are not relegated solely to industry and Big Tech – who are guide by a different set of commercial priorities– particularly for research areas associated with large human costs (in labour and social terms). 
  </p>




  
<div class="card-body-title">
  <div class="col-md-6">
    <h3 class="font-weight-bold mb-4 serif-font">Recent News:</h3>
  </div>
</div>

  <p class="article-post serif-font">   I organise the Natural Language & Information Processing (NLIP) Seminars in the Department of Computer Science & Technology, University of Cambridge. </p>
<ul>
  <li><a href="{{site.baseurl}}/nlip-mich24">NLIP Seminars (Michaelmas 2024): Takeaways and Personal Reflections</a></li>
  <li><a href="{{site.baseurl}}/nlip-lent25">NLIP Seminars (Lent 2025): Takeaways and Personal Reflections</a></li>
</ul>

  
<!-- 
<div class="rounded mb-5 hero">
  <div class="row align-items-center justify-content-between">
      <div class="col-md-6 text-left pl-0 pl-lg-4">
 </p>
    <div class="d-flex justify-content-center gap-3">
    </div>
  </div>
  <div class="col-md-6">
         </div>
</div>
</div>
-->




  
    <h3 class="font-weight-bold mb-4 serif-font">Handouts & Teaching Materials:</h3>


    <p class="article-post serif-font">   I delivered my first guest lecture in November 2024 for an MPhil course in the University of Cambridge with Prof Buttery and Dr Fermín Moscoso del Prado Martín on Language Model Evaluation – aged 22, this was a great opportunity and privilege so early in my “formal” academic career. In Lent 2025, I am the Teaching Assistant (a new role equivalent to Lead Demonstrator) for CST IA Machine Learning & Real World Data and am a supervisor for Machine Learning & Bayesian Inference (MBLI) [CST Part II]. I am supervising an MPhil Project on Language Model Stability. </p>


  <div class="rounded mb-5 hero">
  <div class="row align-items-center justify-content-between">
     <a href="{{site.baseurl}}/slm" class="btn btn-dark text-white px-5 btn-lg"> </a>
  <p class="article-post serif-font">  <u><a href={{site.baseurl}}/tripos>Cambridge Tripos Page</a></u> contains a (non-exhaustive) set of resources for the Computer Science and Linguistics Triposes.</p>
  </div>
  </div>



    
    <!-- Pagination -->
    <div class="bottompagination">
      <span class="navigation" role="navigation">
          {% include pagination.html %}
      </span>
    </div>

<br>

